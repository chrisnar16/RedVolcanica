{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":20954,"status":"ok","timestamp":1651839564544,"user":{"displayName":"Christian Mauricio Yépez Escalante","userId":"15071697346296237740"},"user_tz":300},"id":"hhg4evFt81xb","outputId":"4e25922f-0e98-4adf-8443-f499326f4d23"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n","/content/drive/MyDrive/Tesis/Git/RedVolcanica\n","True\n"]}],"source":["try:\n","    from google.colab import drive\n","    drive.mount('/content/drive')\n","    py_file_location = \"/content/drive/MyDrive/Tesis/Git/RedVolcanica\"\n","    %cd \"{py_file_location}\" \n","    IN_COLAB = True\n","except:\n","    IN_COLAB = False\n","print(IN_COLAB)  "]},{"cell_type":"code","execution_count":2,"metadata":{"id":"ti01SFhJ9mzS","executionInfo":{"status":"ok","timestamp":1651839578506,"user_tz":300,"elapsed":8952,"user":{"displayName":"Christian Mauricio Yépez Escalante","userId":"15071697346296237740"}}},"outputs":[],"source":["import importlib\n","\n","import RedFunciones.visualizacion as visualizacion\n","\n","import RedFunciones.Generador as Generador\n","import RedFunciones.Discriminador as Discriminador\n","import RedFunciones.auxiliares as auxiliares\n","import RedFunciones.DataloaderVol as DataloaderVol\n","import RedFunciones.Checkpoint as Checkpoint\n","\n","importlib.reload(auxiliares)\n","importlib.reload(visualizacion)\n","importlib.reload(Generador)\n","importlib.reload(Discriminador)\n","importlib.reload(DataloaderVol)\n","importlib.reload(Checkpoint)\n","\n","import Auxiliares.BinaryAccuracy as bin_acc"]},{"cell_type":"code","execution_count":3,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":523,"status":"ok","timestamp":1651839596211,"user":{"displayName":"Christian Mauricio Yépez Escalante","userId":"15071697346296237740"},"user_tz":300},"id":"d-u2HlmUUDqF","outputId":"ff470610-7c4e-4afe-ae26-56d4bf9ebf5e"},"outputs":[{"output_type":"stream","name":"stdout","text":["/content/drive/MyDrive/Tesis/Git/RedVolcanica\n"]}],"source":["import torch\n","from torch import nn\n","import torchaudio\n","\n","from tqdm.auto import tqdm # Progress bar\n","\n","from torchvision import transforms\n","from torchvision.utils import make_grid\n","from torchvision.datasets import MNIST #noned\n","\n","from torch.utils.data import DataLoader\n","import matplotlib.pyplot as plt\n","\n","torch.manual_seed(0) # Set for our testing purposes, please do not change!\n","\n","import os\n","cwd = os.getcwd()\n","print(cwd)\n","\n","cuda0 = torch.device('cuda:3')\n","cpu = 'cpu'\n","device = torch.device(cuda0 if torch.cuda.is_available() else cpu)"]},{"cell_type":"markdown","metadata":{"id":"Z5T2lKzKDm3W"},"source":["criterion: the loss function\n","\n","n_epochs: the number of times you iterate through the entire \n","dataset when training\n","\n","z_dim: the dimension of the noise vector\n","\n","display_step: how often to display/visualize the images\n","\n","batch_size: the number of images per forward/backward pass\n","\n","lr: the learning rate\n","\n","device: the device type"]},{"cell_type":"code","execution_count":4,"metadata":{"id":"3yenniLFFfEP","executionInfo":{"status":"ok","timestamp":1651839599054,"user_tz":300,"elapsed":210,"user":{"displayName":"Christian Mauricio Yépez Escalante","userId":"15071697346296237740"}}},"outputs":[],"source":["h5_filename = cwd + '/baseh5/baseSR10.h5'\n","nombre = 'master'\n","nombre_carga = 'master-2022-04-04.pt'\n","\n","mnist_shape = (1, 129, 33) #tamaño imagen\n","n_classes = 2 # numkero etiquetas\n","z_dim = 64\n","\n","n_epochs = 25\n","display_step = 50\n","batch_size = 16\n","\n","lrg = 0.0001\n","beta_1_g = 0.5\n","beta_2_g = 0.999\n","c_lambda_g = 10\n","repeats_g = 3\n","\n","lrd = 0.00006\n","beta_1_d = 0.5\n","beta_2_d = 0.999\n","crit_repeats = 1\n","c_lambda = 20\n","\n","mean = 0\n","std = 1\n","\n","guardar = False\n","cargar = False\n","save_steep = 1\n","\n","epoch_temp = -1\n","gen_loss_temp = torch.empty([])\n","disc_loss_temp = torch.empty([]) "]},{"cell_type":"code","execution_count":5,"metadata":{"id":"oa9AxW6PLuEN","executionInfo":{"status":"ok","timestamp":1651839601528,"user_tz":300,"elapsed":204,"user":{"displayName":"Christian Mauricio Yépez Escalante","userId":"15071697346296237740"}}},"outputs":[],"source":["mean_p = False\n","\n","regularizaM = True\n","reg_lambda = 0.001"]},{"cell_type":"code","execution_count":6,"metadata":{"id":"2jS27fJSj705","executionInfo":{"status":"ok","timestamp":1651839603400,"user_tz":300,"elapsed":5,"user":{"displayName":"Christian Mauricio Yépez Escalante","userId":"15071697346296237740"}}},"outputs":[],"source":["if mean_p:\n","    myDataA = DataloaderVol.VolcanoDatasetH5(h5_filename)\n","    dataloaderA = DataLoader(\n","        myDataA,\n","        batch_size=batch_size,\n","        shuffle=True)\n","\n","    def get_mean_and_std(dataloader):\n","        channels_sum, channels_squared_sum, num_batches = 0, 0, 0\n","        for data, _ in dataloader:\n","            # Mean over batch, height and width, but not over the channels\n","            channels_sum += torch.mean(data, dim=[0,2,3])\n","            channels_squared_sum += torch.mean(data**2, dim=[0,2,3])\n","            num_batches += 1    \n","        mean = channels_sum / num_batches\n","        # std = sqrt(E[X^2] - (E[X])^2)\n","        std = (channels_squared_sum / num_batches - mean ** 2) ** 0.5\n","        return mean.item(), std.item()\n","\n","    mean, std = get_mean_and_std(dataloaderA)\n","    print(mean)\n","    print(std)"]},{"cell_type":"code","execution_count":7,"metadata":{"id":"eh7DtKHEHTY3","executionInfo":{"status":"ok","timestamp":1651839607571,"user_tz":300,"elapsed":491,"user":{"displayName":"Christian Mauricio Yépez Escalante","userId":"15071697346296237740"}}},"outputs":[],"source":["#cargar datos\n","if mean_p:\n","    transform = transforms.Compose([\n","        transforms.Normalize((mean,), (std,)),\n","        #transforms.ToTensor(),\n","        #torchaudio.transforms.MelScale(sample_rate=50, n_stft=129)\n","    ])    \n","else:\n","    transform = transforms.Compose([\n","        #transforms.ToTensor(),\n","        #transforms.Normalize((mean,), (std,)),\n","        #torchaudio.transforms.MelScale(sample_rate=50, n_stft=129)\n","    ])    \n","myData = DataloaderVol.VolcanoDatasetH5(h5_filename, transform)\n","\n","dataloader = DataLoader(\n","    myData,\n","    batch_size=batch_size,\n","    shuffle=True)\n","\n","#a, b = next(iter(dataloader))\n","#print(a)"]},{"cell_type":"code","execution_count":8,"metadata":{"id":"BQRA9L7sJVPU","executionInfo":{"status":"ok","timestamp":1651839613117,"user_tz":300,"elapsed":3463,"user":{"displayName":"Christian Mauricio Yépez Escalante","userId":"15071697346296237740"}}},"outputs":[],"source":["generator_input_dim, discriminator_im_chan = auxiliares.get_input_dimensions(z_dim, mnist_shape, n_classes)\n","\n","gen = Generador.Generator(input_dim=generator_input_dim).to(device)\n","gen_opt = torch.optim.Adam(gen.parameters(), lr=lrg, betas=(beta_1_g, beta_2_g))\n","disc = Discriminador.Discriminator(im_chan=discriminator_im_chan).to(device)\n","disc_opt = torch.optim.Adam(disc.parameters(), lr=lrd, betas=(beta_1_d, beta_2_d))\n","gen = gen.apply(auxiliares.weights_init)\n","disc = disc.apply(auxiliares.weights_init)\n","\n","gen.train()\n","disc.train()\n","        \n","metric = bin_acc.BinaryAccuracy()"]},{"cell_type":"code","execution_count":9,"metadata":{"id":"q1Lm_AO6LuEP","executionInfo":{"status":"ok","timestamp":1651839615635,"user_tz":300,"elapsed":200,"user":{"displayName":"Christian Mauricio Yépez Escalante","userId":"15071697346296237740"}}},"outputs":[],"source":["def get_gradient(crit, real, fake, epsilon):\n","    '''\n","    Return the gradient of the critic's scores with respect to mixes of real and fake images.\n","    Parameters:\n","        crit: the critic model\n","        real: a batch of real images\n","        fake: a batch of fake images\n","        epsilon: a vector of the uniformly random proportions of real/fake per mixed image\n","    Returns:\n","        gradient: the gradient of the critic's scores, with respect to the mixed image\n","    '''\n","    # Mix the images together\n","    mixed_images = real * epsilon + fake * (1 - epsilon)\n","\n","    # Calculate the critic's scores on the mixed images\n","    mixed_scores = crit(mixed_images)\n","    \n","    # Take the gradient of the scores with respect to the images\n","    gradient = torch.autograd.grad(\n","        # Note: You need to take the gradient of outputs with respect to inputs.\n","        # This documentation may be useful, but it should not be necessary:\n","        # https://pytorch.org/docs/stable/autograd.html#torch.autograd.grad\n","        #### START CODE HERE ####\n","        inputs=mixed_images,\n","        outputs=mixed_scores,\n","        #### END CODE HERE ####\n","        # These other parameters have to do with the pytorch autograd engine works\n","        grad_outputs=torch.ones_like(mixed_scores), \n","        create_graph=True,\n","        retain_graph=True,\n","    )[0]\n","    return gradient\n","\n","def gradient_penalty(gradient):\n","    '''\n","    Return the gradient penalty, given a gradient.\n","    Given a batch of image gradients, you calculate the magnitude of each image's gradient\n","    and penalize the mean quadratic distance of each magnitude to 1.\n","    Parameters:\n","        gradient: the gradient of the critic's scores, with respect to the mixed image\n","    Returns:\n","        penalty: the gradient penalty\n","    '''\n","    # Flatten the gradients so that each row captures one image\n","    gradient = gradient.view(len(gradient), -1)\n","\n","    # Calculate the magnitude of every row\n","    gradient_norm = gradient.norm(2, dim=1)\n","    \n","    # Penalize the mean squared distance of the gradient norms from 1\n","    #### START CODE HERE ####\n","    penalty = torch.mean((gradient_norm - 1)**2)\n","    #### END CODE HERE ####\n","    return penalty\n","\n","# UNQ_C3 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)\n","# GRADED FUNCTION: get_gen_loss\n","def get_gen_loss(crit_fake_pred):\n","    '''\n","    Return the loss of a generator given the critic's scores of the generator's fake images.\n","    Parameters:\n","        crit_fake_pred: the critic's scores of the fake images\n","    Returns:\n","        gen_loss: a scalar loss value for the current batch of the generator\n","    '''\n","    #### START CODE HERE ####\n","    gen_loss = -1. * torch.mean(crit_fake_pred)\n","    #### END CODE HERE ####\n","    return gen_loss\n","\n","def get_crit_loss(crit_fake_pred, crit_real_pred, gp, c_lambda):\n","    '''\n","    Return the loss of a critic given the critic's scores for fake and real images,\n","    the gradient penalty, and gradient penalty weight.\n","    Parameters:\n","        crit_fake_pred: the critic's scores of the fake images\n","        crit_real_pred: the critic's scores of the real images\n","        gp: the unweighted gradient penalty\n","        c_lambda: the current weight of the gradient penalty \n","    Returns:\n","        crit_loss: a scalar for the critic's loss, accounting for the relevant factors\n","    '''\n","    #### START CODE HERE ####\n","    crit_loss = torch.mean(crit_fake_pred) - torch.mean(crit_real_pred) + c_lambda * gp\n","    #### END CODE HERE ####\n","    return crit_loss"]},{"cell_type":"code","execution_count":10,"metadata":{"id":"Uwf8nslcSBWF","executionInfo":{"status":"ok","timestamp":1651839618518,"user_tz":300,"elapsed":208,"user":{"displayName":"Christian Mauricio Yépez Escalante","userId":"15071697346296237740"}}},"outputs":[],"source":["if cargar:\n","    compelto = Checkpoint.DIRECTORY + nombre_carga\n","    checkpoint = torch.load(compelto)\n","    gen.load_state_dict(checkpoint['gen'])\n","    disc.load_state_dict(checkpoint['disc'])\n","    gen_opt.load_state_dict(checkpoint['gen_opt'])\n","    disc_opt.load_state_dict(checkpoint['disc_opt'])\n","    epoch_temp = checkpoint['epoch']\n","    gen_loss_temp = checkpoint['gen_loss']\n","    disc_loss_temp = checkpoint['dis_loss']\n","    print('Modelo cargado')\n","    print('epoch: ' + str(epoch_temp))"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true,"base_uri":"https://localhost:8080/","height":86,"referenced_widgets":["e067a906ed8a4564900464814a86301b","4239864ea47c46b682019c3da0952567","f76d325a48864b80bc1549ecc6999d85","652d668ed6274d58bb995d56416706b5","33d334d1ba374f669bc49a731f9e8ee3","7619fdbb50934d5895a16013ae42d90a","6de60784b83d4b3caa84da5ee1f8df53","31a3b4a5714541dda785bb22963525e9","7a214b4a7e894a0a9d9a18ce3a90cb3d","ed786cf395714b5ca82cdc28a1374c1f","4dcd5aa642b9420da6acd3bcaa44bb3e"]},"id":"5dKjX-BpKq2s","outputId":"38c176b8-e3d6-4d90-bc7f-4b95c07e3239","scrolled":false},"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch: 0\n"]},{"output_type":"display_data","data":{"text/plain":["  0%|          | 0/385 [00:00<?, ?it/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e067a906ed8a4564900464814a86301b"}},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Red Funcionando\n"]}],"source":["generator_losses = []\n","discriminator_losses = []\n","discriminator_losses_real = []\n","discriminator_losses_fake = []\n","metric_real = []\n","metric_fake = [] \n","critic_losses = []\n","cur_step = 0\n","\n","for epoch in range(epoch_temp + 1, n_epochs):\n","    print('Epoch: ' + str(epoch))\n","    # Dataloader returns the batches and the labels\n","    for real, labels in tqdm(dataloader):\n","        cur_batch_size = len(real)\n","        # Flatten the batch of real images from the dataset\n","        real = real.to(device)              \n","        mean_iteration_critic_loss = 0\n","        mean_metric_fake = 0\n","        mean_metric_real = 0\n","        for _ in range(crit_repeats):\n","            one_hot_labels = auxiliares.get_one_hot_labels(labels.to(device), n_classes)\n","            image_one_hot_labels = one_hot_labels[:, :, None, None]\n","            image_one_hot_labels = image_one_hot_labels.repeat(1, 1, mnist_shape[1], mnist_shape[2])\n","\n","            ### Update discriminator ###\n","            # Zero out the discriminator gradients\n","            disc_opt.zero_grad()\n","            # Get noise corresponding to the current batch_size \n","            fake_noise = auxiliares.get_noise(cur_batch_size, z_dim, device=device)\n","        \n","            # Now you can get the images from the generator\n","            # Steps: 1) Combine the noise vectors and the one-hot labels for the generator\n","            #        2) Generate the conditioned fake images\n","       \n","            noise_and_labels = auxiliares.combine_vectors(fake_noise, one_hot_labels)\n","            fake = gen(noise_and_labels)#gen.forward(noise_and_labels) no usar\n","\n","            # Now you can get the predictions from the discriminator\n","            # Steps: 1) Create the input for the discriminator\n","            #           a) Combine the fake images with image_one_hot_labels, \n","            #              remember to detach the generator (.detach()) so you do not backpropagate through it\n","            #           b) Combine the real images with image_one_hot_labels\n","            #        2) Get the discriminator's prediction on the fakes as disc_fake_pred\n","            #        3) Get the discriminator's prediction on the reals as disc_real_pred\n","        \n","            fake_image_and_labels = auxiliares.combine_vectors(fake, image_one_hot_labels)\n","            real_image_and_labels = auxiliares.combine_vectors(real, image_one_hot_labels)\n","            disc_fake_pred = disc(fake_image_and_labels.detach())\n","            disc_real_pred = disc(real_image_and_labels)\n","        \n","            epsilon = torch.rand(len(real), 1, 1, 1, device=device, requires_grad=True)\n","            gradient = get_gradient(disc, real_image_and_labels, fake_image_and_labels.detach(), epsilon)\n","            gp = gradient_penalty(gradient)\n","            crit_loss = get_crit_loss(disc_fake_pred, disc_real_pred, gp, c_lambda)\n","            \n","             # Keep track of the average critic loss in this batch\n","            mean_iteration_critic_loss += crit_loss.item() / crit_repeats    \n","            mean_metric_fake += (metric(disc_fake_pred, torch.zeros_like(disc_fake_pred)).item()) / crit_repeats    \n","            mean_metric_real += (metric(disc_real_pred, torch.ones_like(disc_real_pred)).item()) / crit_repeats    \n","            if regularizaM:\n","                l2_reg = None\n","                for W in disc.parameters():\n","                    if l2_reg is None:\n","                        l2_reg = W.norm(2)\n","                    else:\n","                        l2_reg = l2_reg + W.norm(2)\n","                crit_loss = crit_loss +  l2_reg * reg_lambda\n","                       \n","            crit_loss.backward(retain_graph=True)\n","            disc_opt.step() \n","        \n","        critic_losses += [mean_iteration_critic_loss]\n","        metric_fake += [mean_metric_fake]\n","        metric_real += [mean_metric_real]\n","\n","\n","        ### Update generator ###\n","        # Zero out the generator gradients\n","        mean_iteration_gen_loss = 0\n","        for _ in range(repeats_g):\n","            gen_opt.zero_grad()\n","\n","            fake_noise_2 = auxiliares.get_noise(cur_batch_size, z_dim, device=device)\n","            noise_and_labels_2 = auxiliares.combine_vectors(fake_noise_2, one_hot_labels)\n","            fake_2 = gen(noise_and_labels_2)#gen.forward(noise_and_labels) no usar\n","            fake_image_and_labels = auxiliares.combine_vectors(fake_2, image_one_hot_labels)\n","            # This will error if you didn't concatenate your labels to your image correctly\n","            disc_fake_pred = disc(fake_image_and_labels)\n","            gen_loss = get_gen_loss(disc_fake_pred)\n","            mean_iteration_gen_loss += gen_loss.item() / repeats_g    \n","            gen_loss.backward()\n","            gen_opt.step()\n","            \n","\n","        # Keep track of the generator losses\n","        generator_losses += [mean_iteration_gen_loss]\n","        #\n","\n","        if cur_step % display_step == 0 and cur_step > 0:\n","            gen_mean = sum(generator_losses[-display_step:]) / display_step\n","            disc_mean = sum(discriminator_losses[-display_step:]) / display_step\n","            print(f\"Step {cur_step}: Generator loss: {gen_mean}, discriminator loss: {disc_mean}\")\n","            print(f\"Espectrogramas Generados\")\n","            visualizacion.show_tensor_images(torch.transpose(fake, 2, 3), size=(1, 129, 33))\n","            print(f\"Espectrogramas Reales\")\n","            visualizacion.show_tensor_images(torch.transpose(real, 2, 3), size=(1, 129, 33))\n","            visualizacion.show_time_domine_images(torch.transpose(real, 2, 3), size=(1, 129, 33), std=std, mean=mean, real = True)\n","            visualizacion.show_time_domine_images(torch.transpose(fake, 2, 3), size=(1, 129, 33), std=std, mean=mean, real = False)\n","            step_bins = 20\n","            x_axis = sorted([i * step_bins for i in range(len(generator_losses) // step_bins)] * step_bins)\n","            num_examples = (len(generator_losses) // step_bins) * step_bins\n","            plt.plot(\n","                range(num_examples // step_bins), \n","                torch.Tensor(generator_losses[:num_examples]).view(-1, step_bins).mean(1),\n","                label=\"Generator Loss\"\n","            )\n","            plt.plot(\n","                range(num_examples // step_bins), \n","                torch.Tensor(critic_losses[:num_examples]).view(-1, step_bins).mean(1),\n","                label=\"Discriminator Loss\"\n","            )\n","            plt.legend()\n","            \n","            plt.show()\n","\n","            plt.plot(\n","                range(num_examples // step_bins), \n","                torch.Tensor(metric_real[:num_examples]).view(-1, step_bins).mean(1),\n","                label=\"METRIC REAL\"\n","            )\n","            plt.plot(\n","                range(num_examples // step_bins), \n","                torch.Tensor(metric_fake[:num_examples]).view(-1, step_bins).mean(1),\n","                label=\"METRIC FAKE\"\n","            )\n","            plt.legend()\n","            plt.show()\n","        elif cur_step == 0:\n","            print(\"Red Funcionando\")\n","        cur_step += 1 \n","    if((guardar and epoch % save_steep == 0) or epoch == n_epochs - 1):\n","        Checkpoint.save_weighs(gen, disc, gen_opt, disc_opt, epoch, gen_loss, disc_loss, nombre)\n","        print('epoch guardada')                "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"gT-QnN2u-rvp"},"outputs":[],"source":["#gen.state_dict()\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"RwDyX-xl7m1C"},"outputs":[],"source":["from torchvision import models\n","from torchsummary import summary\n","\n","summary(gen, (66,1,1))\n","\n","summary(disc, (3,129,33))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"boSIiJ2rj71A"},"outputs":[],"source":["gen.eval()\n","disc.eval()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"hUTT5mStj71B"},"outputs":[],"source":["import PrePross.grifflin as grifflin\n","import numpy as np\n","\n","examples = 1\n","\n","for i in range(examples):\n","    x = torch.tensor([1])\n","    one_hot_labels = auxiliares.get_one_hot_labels(x.to(device), n_classes)\n","    fake_noise = auxiliares.get_noise(1, z_dim, device=device)\n","    noise_and_labels = auxiliares.combine_vectors(fake_noise, one_hot_labels)\n","    fake = gen(noise_and_labels)\n","    fake = fake.cpu().detach().numpy()\n","    \n","    #fake = real[1].cpu().detach().numpy()\n","    #fake = np.expand_dims(fake, axis=1)\n","    \n","    #real, label = myData.__getitem__(852)\n","    #fake = real.cpu().detach().numpy()\n","    #fake = np.expand_dims(fake, axis=1)\n","    \n","    fake = fake * std + mean\n","    samplerate = 50\n","    timee, muestra_rec=grifflin.reconstruir_señal_generador(fake, 1000, samplerate)\n","    muestra_rec = np.squeeze(muestra_rec)\n","    tamaño = len(muestra_rec) / samplerate\n","    time = np.linspace(0., tamaño, len(muestra_rec))\n","    plt.plot(time,muestra_rec)\n","    plt.title(\"Señal Recuperada\")\n","    plt.xlabel(\"Time [s]\")\n","    plt.ylabel(\"Amplitude\")\n","    plt.show()\n","    "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"0v73cICPj71C"},"outputs":[],"source":["print(real)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"yIzeVMWuj71C"},"outputs":[],"source":[""]}],"metadata":{"accelerator":"GPU","colab":{"collapsed_sections":[],"name":"Red-W.ipynb","provenance":[]},"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.12"},"widgets":{"application/vnd.jupyter.widget-state+json":{"e067a906ed8a4564900464814a86301b":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_4239864ea47c46b682019c3da0952567","IPY_MODEL_f76d325a48864b80bc1549ecc6999d85","IPY_MODEL_652d668ed6274d58bb995d56416706b5"],"layout":"IPY_MODEL_33d334d1ba374f669bc49a731f9e8ee3"}},"4239864ea47c46b682019c3da0952567":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_7619fdbb50934d5895a16013ae42d90a","placeholder":"​","style":"IPY_MODEL_6de60784b83d4b3caa84da5ee1f8df53","value":"  0%"}},"f76d325a48864b80bc1549ecc6999d85":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"","description":"","description_tooltip":null,"layout":"IPY_MODEL_31a3b4a5714541dda785bb22963525e9","max":385,"min":0,"orientation":"horizontal","style":"IPY_MODEL_7a214b4a7e894a0a9d9a18ce3a90cb3d","value":1}},"652d668ed6274d58bb995d56416706b5":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_ed786cf395714b5ca82cdc28a1374c1f","placeholder":"​","style":"IPY_MODEL_4dcd5aa642b9420da6acd3bcaa44bb3e","value":" 1/385 [02:24&lt;15:22:44, 144.18s/it]"}},"33d334d1ba374f669bc49a731f9e8ee3":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"7619fdbb50934d5895a16013ae42d90a":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"6de60784b83d4b3caa84da5ee1f8df53":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"31a3b4a5714541dda785bb22963525e9":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"7a214b4a7e894a0a9d9a18ce3a90cb3d":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"ed786cf395714b5ca82cdc28a1374c1f":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"4dcd5aa642b9420da6acd3bcaa44bb3e":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"nbformat":4,"nbformat_minor":0}