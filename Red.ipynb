{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Red.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyPDRVEmegpoEp2tiqFDzxkD"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"hhg4evFt81xb","executionInfo":{"status":"ok","timestamp":1637247612027,"user_tz":300,"elapsed":179,"user":{"displayName":"Christian Mauricio Yépez Escalante","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiPAU1qTofut__rEh6qCzvBRAI1UOFlzBDDQOqKIA=s64","userId":"15071697346296237740"}},"outputId":"9a3d8b3c-ef18-4301-fef8-9b4272ac82cd"},"source":["from google.colab import drive\n","drive.mount('/content/drive')\n","py_file_location = \"/content/drive/MyDrive/Tesis/Git/RedVolcanica\"\n","%cd \"{py_file_location}\" "],"execution_count":71,"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n","/content/drive/MyDrive/Tesis/Git/RedVolcanica\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ti01SFhJ9mzS","executionInfo":{"status":"ok","timestamp":1637247613758,"user_tz":300,"elapsed":182,"user":{"displayName":"Christian Mauricio Yépez Escalante","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiPAU1qTofut__rEh6qCzvBRAI1UOFlzBDDQOqKIA=s64","userId":"15071697346296237740"}},"outputId":"1ab91413-cdf3-4d58-b648-fe8dae58278e"},"source":["import importlib\n","\n","import RedFunciones.visualizacion as visualizacion\n","import RedFunciones.Generador as Generador\n","import RedFunciones.Discriminador as Discriminador\n","import RedFunciones.auxiliares as auxiliares\n","import RedFunciones.DataloaderVol as DataloaderVol\n","import RedFunciones.Checkpoint as Checkpoint\n","\n","importlib.reload(auxiliares)\n","importlib.reload(Generador)\n","importlib.reload(DataloaderVol)\n","importlib.reload(Checkpoint)"],"execution_count":72,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<module 'RedFunciones.Checkpoint' from '/content/drive/My Drive/Tesis/Git/RedVolcanica/RedFunciones/Checkpoint.py'>"]},"metadata":{},"execution_count":72}]},{"cell_type":"code","metadata":{"id":"d-u2HlmUUDqF","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1637247615483,"user_tz":300,"elapsed":200,"user":{"displayName":"Christian Mauricio Yépez Escalante","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiPAU1qTofut__rEh6qCzvBRAI1UOFlzBDDQOqKIA=s64","userId":"15071697346296237740"}},"outputId":"80534829-168b-4ae2-a3f7-226a65811cf4"},"source":["import torch\n","from torch import nn\n","\n","from tqdm.auto import tqdm # Progress bar\n","\n","from torchvision import transforms\n","from torchvision.utils import make_grid\n","from torchvision.datasets import MNIST #noned\n","\n","from torch.utils.data import DataLoader\n","import matplotlib.pyplot as plt\n","\n","torch.manual_seed(0) # Set for our testing purposes, please do not change!\n","\n"],"execution_count":73,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<torch._C.Generator at 0x7f5702f75550>"]},"metadata":{},"execution_count":73}]},{"cell_type":"markdown","metadata":{"id":"4wj4N812F0H-"},"source":["criterion: the loss function\n","\n","n_epochs: the number of times you iterate through the entire \n","dataset when training\n","\n","z_dim: the dimension of the noise vector\n","\n","display_step: how often to display/visualize the images\n","\n","batch_size: the number of images per forward/backward pass\n","\n","lr: the learning rate\n","\n","device: the device type"]},{"cell_type":"code","metadata":{"id":"3yenniLFFfEP","executionInfo":{"status":"ok","timestamp":1637248718982,"user_tz":300,"elapsed":188,"user":{"displayName":"Christian Mauricio Yépez Escalante","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiPAU1qTofut__rEh6qCzvBRAI1UOFlzBDDQOqKIA=s64","userId":"15071697346296237740"}}},"source":["mnist_shape = (1, 129, 33) #tamaño imagen\n","n_classes = 2 # numkero etiquetas\n","cuda0 = torch.device('cuda:0')\n","cpu = 'cpu'\n","\n","device = torch.device(cuda0 if torch.cuda.is_available() else cpu)\n","\n","criterion = nn.BCEWithLogitsLoss()\n","n_epochs = 5\n","z_dim = 64\n","display_step = 500\n","batch_size = 30\n","lr = 0.0002\n","\n","nombre = 'prueba'\n","nombre_carga = 'prueba-2021-11-18.pt'\n","guardar = True\n","cargar = False\n","save_steep = 1\n","\n","epoch_temp = -1\n","gen_loss_temp = torch.empty([])\n","disc_loss_temp = torch.empty([]) "],"execution_count":102,"outputs":[]},{"cell_type":"code","metadata":{"id":"eh7DtKHEHTY3","executionInfo":{"status":"ok","timestamp":1637248721195,"user_tz":300,"elapsed":180,"user":{"displayName":"Christian Mauricio Yépez Escalante","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiPAU1qTofut__rEh6qCzvBRAI1UOFlzBDDQOqKIA=s64","userId":"15071697346296237740"}}},"source":["# transform talves a funcion\n","transform = transforms.Compose([\n","    transforms.ToTensor(),\n","    transforms.Normalize((0.5,), (0.5,)),\n","])\n","\n","#cargar datos\n","dir_nam = '/content/drive/MyDrive/Tesis/Red/basedesglosada/'\n","json_nam = '/content/drive/MyDrive/Tesis/Red/data.json'\n","h5_filename = '/content/drive/MyDrive/Tesis/Git/RedVolcanica/baseh5/data1.h5'\n","\n","myData = DataloaderVol.VolcanoDatasetH5(h5_filename)\n","\n","dataloader = DataLoader(\n","    myData,\n","    batch_size=batch_size,\n","    shuffle=True)"],"execution_count":103,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"iCU_Hi1dzff9"},"source":["# New Section"]},{"cell_type":"code","metadata":{"id":"BQRA9L7sJVPU","executionInfo":{"status":"ok","timestamp":1637248723006,"user_tz":300,"elapsed":178,"user":{"displayName":"Christian Mauricio Yépez Escalante","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiPAU1qTofut__rEh6qCzvBRAI1UOFlzBDDQOqKIA=s64","userId":"15071697346296237740"}}},"source":["generator_input_dim, discriminator_im_chan = auxiliares.get_input_dimensions(z_dim, mnist_shape, n_classes)\n","\n","gen = Generador.Generator(input_dim=generator_input_dim).to(device)\n","gen_opt = torch.optim.Adam(gen.parameters(), lr=lr)\n","disc = Discriminador.Discriminator(im_chan=discriminator_im_chan).to(device)\n","disc_opt = torch.optim.Adam(disc.parameters(), lr=lr)\n","\n","gen = gen.apply(auxiliares.weights_init)\n","disc = disc.apply(auxiliares.weights_init)"],"execution_count":104,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Uwf8nslcSBWF","executionInfo":{"status":"ok","timestamp":1637249497077,"user_tz":300,"elapsed":197,"user":{"displayName":"Christian Mauricio Yépez Escalante","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiPAU1qTofut__rEh6qCzvBRAI1UOFlzBDDQOqKIA=s64","userId":"15071697346296237740"}},"outputId":"e9e4e8fc-de32-4492-8768-ea6f2931117d"},"source":["\n","if cargar:\n","  compelto = Checkpoint.DIRECTORY + nombre_carga\n","  checkpoint = torch.load(compelto)\n","  gen.load_state_dict(checkpoint['gen'])\n","  disc.load_state_dict(checkpoint['disc'])\n","  gen_opt.load_state_dict(checkpoint['gen_opt'])\n","  disc_opt.load_state_dict(checkpoint['disc_opt'])\n","  epoch_temp = checkpoint['epoch']\n","  gen_loss_temp = checkpoint['gen_loss']\n","  disc_loss_temp = checkpoint['dis_loss']\n","  print('Modelo cargado')\n","  print('epoch: ' + str(epoch_temp))"],"execution_count":107,"outputs":[{"output_type":"stream","name":"stdout","text":["Modelo cargado\n","epoch: 4\n"]}]},{"cell_type":"code","metadata":{"id":"5dKjX-BpKq2s","executionInfo":{"status":"ok","timestamp":1637249499225,"user_tz":300,"elapsed":193,"user":{"displayName":"Christian Mauricio Yépez Escalante","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiPAU1qTofut__rEh6qCzvBRAI1UOFlzBDDQOqKIA=s64","userId":"15071697346296237740"}}},"source":["cur_step = 0\n","generator_losses = []\n","discriminator_losses = []\n","\n","#UNIT TEST NOTE: Initializations needed for grading\n","noise_and_labels = False\n","fake = False\n","\n","fake_image_and_labels = False\n","real_image_and_labels = False\n","disc_fake_pred = False\n","disc_real_pred = False\n","\n","\n","for epoch in range(epoch_temp + 1, n_epochs):\n","    print('Epoch: ' + str(epoch))\n","    # Dataloader returns the batches and the labels\n","    for real, labels in tqdm(dataloader):\n","        cur_batch_size = len(real)\n","        # Flatten the batch of real images from the dataset\n","        real = real.unsqueeze(1)\n","        real = real.to(device)\n","\n","        one_hot_labels = auxiliares.get_one_hot_labels(labels.to(device), n_classes)\n","        image_one_hot_labels = one_hot_labels[:, :, None, None]\n","        image_one_hot_labels = image_one_hot_labels.repeat(1, 1, mnist_shape[1], mnist_shape[2])\n","\n","        ### Update discriminator ###\n","        # Zero out the discriminator gradients\n","        disc_opt.zero_grad()\n","        # Get noise corresponding to the current batch_size \n","        fake_noise = auxiliares.get_noise(cur_batch_size, z_dim, device=device)\n","        \n","        # Now you can get the images from the generator\n","        # Steps: 1) Combine the noise vectors and the one-hot labels for the generator\n","        #        2) Generate the conditioned fake images\n","       \n","        noise_and_labels = auxiliares.combine_vectors(fake_noise, one_hot_labels)\n","        fake = gen(noise_and_labels)#gen.forward(noise_and_labels) no usar\n","\n","        # Now you can get the predictions from the discriminator\n","        # Steps: 1) Create the input for the discriminator\n","        #           a) Combine the fake images with image_one_hot_labels, \n","        #              remember to detach the generator (.detach()) so you do not backpropagate through it\n","        #           b) Combine the real images with image_one_hot_labels\n","        #        2) Get the discriminator's prediction on the fakes as disc_fake_pred\n","        #        3) Get the discriminator's prediction on the reals as disc_real_pred\n","        \n","        fake_image_and_labels = auxiliares.combine_vectors(fake.detach(), image_one_hot_labels)\n","        real_image_and_labels = auxiliares.combine_vectors(real, image_one_hot_labels)\n","        disc_fake_pred = disc(fake_image_and_labels)\n","        disc_real_pred = disc(real_image_and_labels)       \n","        \n","        disc_fake_loss = criterion(disc_fake_pred, torch.zeros_like(disc_fake_pred))\n","        disc_real_loss = criterion(disc_real_pred, torch.ones_like(disc_real_pred))\n","        disc_loss = (disc_fake_loss + disc_real_loss) / 2\n","        disc_loss.backward(retain_graph=True)\n","        disc_opt.step() \n","\n","        # Keep track of the average discriminator loss\n","        discriminator_losses += [disc_loss.item()]\n","\n","        ### Update generator ###\n","        # Zero out the generator gradients\n","        gen_opt.zero_grad()\n","\n","        fake_image_and_labels = auxiliares.combine_vectors(fake, image_one_hot_labels)\n","        # This will error if you didn't concatenate your labels to your image correctly\n","        disc_fake_pred = disc(fake_image_and_labels)\n","        gen_loss = criterion(disc_fake_pred, torch.ones_like(disc_fake_pred))\n","        gen_loss.backward()\n","        gen_opt.step()\n","\n","        # Keep track of the generator losses\n","        generator_losses += [gen_loss.item()]\n","        #\n","\n","        if cur_step % display_step == 0 and cur_step > 0:\n","            gen_mean = sum(generator_losses[-display_step:]) / display_step\n","            disc_mean = sum(discriminator_losses[-display_step:]) / display_step\n","            print(f\"Step {cur_step}: Generator loss: {gen_mean}, discriminator loss: {disc_mean}\")\n","            visualizacion.show_tensor_images(torch.swapaxes(fake, 2, 3))\n","            visualizacion.show_tensor_images(torch.swapaxes(real, 2, 3))\n","            step_bins = 20\n","            x_axis = sorted([i * step_bins for i in range(len(generator_losses) // step_bins)] * step_bins)\n","            num_examples = (len(generator_losses) // step_bins) * step_bins\n","            plt.plot(\n","                range(num_examples // step_bins), \n","                torch.Tensor(generator_losses[:num_examples]).view(-1, step_bins).mean(1),\n","                label=\"Generator Loss\"\n","            )\n","            plt.plot(\n","                range(num_examples // step_bins), \n","                torch.Tensor(discriminator_losses[:num_examples]).view(-1, step_bins).mean(1),\n","                label=\"Discriminator Loss\"\n","            )\n","            plt.legend()\n","            plt.show()\n","        elif cur_step == 0:\n","            print(\"Red Funcionando\")\n","        cur_step += 1 \n","    if((guardar and epoch % save_steep == 0) or epoch == n_epochs - 1):\n","        Checkpoint.save_weighs(gen, disc, gen_opt, disc_opt, epoch, gen_loss, disc_loss, nombre)\n","        print('epoch guardada')                "],"execution_count":108,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"IvMQtgGa_x6B","executionInfo":{"status":"ok","timestamp":1637204577374,"user_tz":300,"elapsed":178,"user":{"displayName":"Christian Mauricio Yépez Escalante","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiPAU1qTofut__rEh6qCzvBRAI1UOFlzBDDQOqKIA=s64","userId":"15071697346296237740"}},"outputId":"c533387c-6b67-478c-b42d-88a978fb9f78"},"source":["print(gen)"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Generator(\n","  (gen): Sequential(\n","    (0): Sequential(\n","      (0): ConvTranspose2d(66, 256, kernel_size=(5, 3), stride=(2, 2))\n","      (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (2): ReLU(inplace=True)\n","    )\n","    (1): Sequential(\n","      (0): ConvTranspose2d(256, 128, kernel_size=(6, 4), stride=(1, 1))\n","      (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (2): ReLU(inplace=True)\n","    )\n","    (2): Sequential(\n","      (0): ConvTranspose2d(128, 64, kernel_size=(6, 4), stride=(2, 2))\n","      (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (2): ReLU(inplace=True)\n","    )\n","    (3): Sequential(\n","      (0): ConvTranspose2d(64, 32, kernel_size=(6, 4), stride=(2, 2))\n","      (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (2): ReLU(inplace=True)\n","    )\n","    (4): Sequential(\n","      (0): ConvTranspose2d(32, 1, kernel_size=(27, 4), stride=(2, 1))\n","      (1): Tanh()\n","    )\n","  )\n",")\n"]}]},{"cell_type":"code","metadata":{"id":"gT-QnN2u-rvp","executionInfo":{"status":"ok","timestamp":1637249509625,"user_tz":300,"elapsed":203,"user":{"displayName":"Christian Mauricio Yépez Escalante","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiPAU1qTofut__rEh6qCzvBRAI1UOFlzBDDQOqKIA=s64","userId":"15071697346296237740"}}},"source":["#gen.state_dict()"],"execution_count":109,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"RwDyX-xl7m1C","executionInfo":{"status":"ok","timestamp":1637205365017,"user_tz":300,"elapsed":175,"user":{"displayName":"Christian Mauricio Yépez Escalante","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiPAU1qTofut__rEh6qCzvBRAI1UOFlzBDDQOqKIA=s64","userId":"15071697346296237740"}},"outputId":"0900db7f-b8ca-44f1-f7cb-037d3c34a722"},"source":["from torchvision import models\n","from torchsummary import summary\n","\n","summary(gen, (66,1,1))\n","\n","summary(disc, (3,129,33))"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["----------------------------------------------------------------\n","        Layer (type)               Output Shape         Param #\n","================================================================\n","   ConvTranspose2d-1            [-1, 256, 5, 3]         253,696\n","       BatchNorm2d-2            [-1, 256, 5, 3]             512\n","              ReLU-3            [-1, 256, 5, 3]               0\n","   ConvTranspose2d-4           [-1, 128, 10, 6]         786,560\n","       BatchNorm2d-5           [-1, 128, 10, 6]             256\n","              ReLU-6           [-1, 128, 10, 6]               0\n","   ConvTranspose2d-7           [-1, 64, 24, 14]         196,672\n","       BatchNorm2d-8           [-1, 64, 24, 14]             128\n","              ReLU-9           [-1, 64, 24, 14]               0\n","  ConvTranspose2d-10           [-1, 32, 52, 30]          49,184\n","      BatchNorm2d-11           [-1, 32, 52, 30]              64\n","             ReLU-12           [-1, 32, 52, 30]               0\n","  ConvTranspose2d-13           [-1, 1, 129, 33]           3,457\n","             Tanh-14           [-1, 1, 129, 33]               0\n","================================================================\n","Total params: 1,290,529\n","Trainable params: 1,290,529\n","Non-trainable params: 0\n","----------------------------------------------------------------\n","Input size (MB): 0.00\n","Forward/backward pass size (MB): 1.96\n","Params size (MB): 4.92\n","Estimated Total Size (MB): 6.89\n","----------------------------------------------------------------\n","----------------------------------------------------------------\n","        Layer (type)               Output Shape         Param #\n","================================================================\n","            Conv2d-1           [-1, 64, 63, 15]           3,136\n","       BatchNorm2d-2           [-1, 64, 63, 15]             128\n","         LeakyReLU-3           [-1, 64, 63, 15]               0\n","            Conv2d-4           [-1, 128, 30, 6]         131,200\n","       BatchNorm2d-5           [-1, 128, 30, 6]             256\n","         LeakyReLU-6           [-1, 128, 30, 6]               0\n","            Conv2d-7             [-1, 1, 14, 2]           2,049\n","================================================================\n","Total params: 136,769\n","Trainable params: 136,769\n","Non-trainable params: 0\n","----------------------------------------------------------------\n","Input size (MB): 0.05\n","Forward/backward pass size (MB): 1.91\n","Params size (MB): 0.52\n","Estimated Total Size (MB): 2.48\n","----------------------------------------------------------------\n"]}]}]}